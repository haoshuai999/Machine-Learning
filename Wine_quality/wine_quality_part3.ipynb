{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied Machine Learning\n",
    "# HW4\n",
    "\n",
    "### Shuai Hao (sh3831), Eugene M. Joseph (emj2152)\n",
    "\n",
    "## Predict wine quality from review texts\n",
    "This notebook explores models using feature vectors from a trained BERT model using [the wine reviews data](https://www.kaggle.com/zynicide/wine-reviews) from Kaggle. The data were scraped on November 22nd, 2017. For this task, only the wine from the US are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Transformers (bonus / optional) [50pts]\n",
    "Fine-tune a BERT model on the text data alone using the transformers library.\n",
    "How does this model compare to a BoW model, and how does it compare to a model using all features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the prompt for this questions asks us to fine-tune a BERT model, we saw the post on Piazza where Professor Mueller said he was just expecting us to use the pretrained model from HuggingFace, so that's what we've done in this notebook.    \n",
    "\n",
    "Piazza Post Link: https://piazza.com/class/k5phinua89310n?cid=439  \n",
    "**Here's the relevant part**: \"The goal was to use the features as-is without finetuning for this exercise. -AM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0421 22:57:26.878679 4603985344 file_utils.py:57] TensorFlow version 2.0.0-rc0 available.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, Normalizer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, StandardScaler, PolynomialFeatures, FunctionTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"winemag-data-130k-v2.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all[df_all[\"country\"] == \"US\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['all_text'] = df['title'].astype(str) + \" \" +df['description'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduce the running time for this part, we sampled 35% percent of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.35, replace=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['points']\n",
    "X = df['all_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0420 17:21:12.993907 4385304000 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-cased-config.json from cache at /Users/emjoseph/.cache/torch/transformers/774d52b0be7c2f621ac9e64708a8b80f22059f6d0e264e1bdc4f4d71c386c4ea.62ec375c5c2c4c3afe8f83d619fc9000f594972092c9113a1747a472a9936c17\n",
      "I0420 17:21:12.999413 4385304000 configuration_utils.py:319] Model config DistilBertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": null,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"dim\": 768,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tie_weights_\": true,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0420 17:21:13.154350 4385304000 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/emjoseph/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0420 17:21:13.422080 4385304000 filelock.py:274] Lock 5754136464 acquired on /Users/emjoseph/.cache/torch/transformers/e7d48e6fc39207404b4bb02c7e5b18c589596b890807864fe4b46fcd88e54e28.455d944f3d1572ab55ed579849f751cf37f303e3388980a42d94f7cd57a4e331.lock\n",
      "I0420 17:21:13.423171 4385304000 file_utils.py:479] https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-cased-modelcard.json not found in cache or force_download set to True, downloading to /Users/emjoseph/.cache/torch/transformers/tmpfuavsjen\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a0dd1ff47545c1972c8bb78ff9e8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0420 17:21:13.688190 4385304000 file_utils.py:489] storing https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-cased-modelcard.json in cache at /Users/emjoseph/.cache/torch/transformers/e7d48e6fc39207404b4bb02c7e5b18c589596b890807864fe4b46fcd88e54e28.455d944f3d1572ab55ed579849f751cf37f303e3388980a42d94f7cd57a4e331\n",
      "I0420 17:21:13.691084 4385304000 file_utils.py:492] creating metadata file for /Users/emjoseph/.cache/torch/transformers/e7d48e6fc39207404b4bb02c7e5b18c589596b890807864fe4b46fcd88e54e28.455d944f3d1572ab55ed579849f751cf37f303e3388980a42d94f7cd57a4e331\n",
      "I0420 17:21:13.693060 4385304000 filelock.py:318] Lock 5754136464 released on /Users/emjoseph/.cache/torch/transformers/e7d48e6fc39207404b4bb02c7e5b18c589596b890807864fe4b46fcd88e54e28.455d944f3d1572ab55ed579849f751cf37f303e3388980a42d94f7cd57a4e331.lock\n",
      "I0420 17:21:13.693679 4385304000 modelcard.py:161] loading model card file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-cased-modelcard.json from cache at /Users/emjoseph/.cache/torch/transformers/e7d48e6fc39207404b4bb02c7e5b18c589596b890807864fe4b46fcd88e54e28.455d944f3d1572ab55ed579849f751cf37f303e3388980a42d94f7cd57a4e331\n",
      "I0420 17:21:13.694738 4385304000 modelcard.py:205] Model card: {\n",
      "  \"caveats_and_recommendations\": {},\n",
      "  \"ethical_considerations\": {},\n",
      "  \"evaluation_data\": {},\n",
      "  \"factors\": {},\n",
      "  \"intended_use\": {},\n",
      "  \"metrics\": {},\n",
      "  \"model_details\": {},\n",
      "  \"quantitative_analyses\": {},\n",
      "  \"training_data\": {}\n",
      "}\n",
      "\n",
      "I0420 17:21:13.854717 4385304000 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-cased-config.json from cache at /Users/emjoseph/.cache/torch/transformers/774d52b0be7c2f621ac9e64708a8b80f22059f6d0e264e1bdc4f4d71c386c4ea.62ec375c5c2c4c3afe8f83d619fc9000f594972092c9113a1747a472a9936c17\n",
      "I0420 17:21:13.855729 4385304000 configuration_utils.py:319] Model config DistilBertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": null,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"dim\": 768,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tie_weights_\": true,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0420 17:21:13.985244 4385304000 modeling_tf_utils.py:390] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-cased-tf_model.h5 from cache at /Users/emjoseph/.cache/torch/transformers/b682f767644ff4a33fb4b68b33e7616d9755137bc6698bd629a0d2ab1ed3883f.e9a7fccc6d3a446e28bd3e5b94a83c3fefa8f745a33ab861db2f2b05f201ab37.h5\n",
      "I0420 17:21:15.729577 4385304000 modeling_tf_utils.py:432] Layers from pretrained model not used in TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"feature-extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79452     Koehler 2009 Dry Riesling (Santa Ynez Valley) ...\n",
       "29434     The Magpie 2009 Lot Five Merlot (Bennett Valle...\n",
       "12550     Morgan 2013 Metallico Un-Oaked Chardonnay (Mon...\n",
       "78288     Renwood 2012 Red (Amador County) This wine sho...\n",
       "119619    Vine Cliff 2004 Cabernet Sauvignon (Napa Valle...\n",
       "104693    Veramar 2010 Viognier (Virginia) Aromas of toa...\n",
       "18857     Heron 2013 Pinot Noir (Monterey County) Globe-...\n",
       "124337    Keegan 2000 Chardonnay (Russian River Valley) ...\n",
       "51686     Calera 2012 Viognier (Mt. Harlan) As luscious ...\n",
       "79210     Migration 2013 Searby Vineyard Chardonnay (Rus...\n",
       "49749     Early Mountain 2013 Reserve Chardonnay (Virgin...\n",
       "117374    Animale 2014 Maria Carmela Red (Washington) Th...\n",
       "18703     Buena Vista 2013 Arpad's Selection Zinfandel (...\n",
       "104773    Thunder Mountain 1999 Ciardella Vineyard Chard...\n",
       "75262     Alpha Omega 2012 Sauvignon Blanc (Napa Valley)...\n",
       "87176     Migration 2015 Bien Nacido Vineyard Chardonnay...\n",
       "77241     Bruliam 2014 Soberanes Vineyard Pinot Noir (Sa...\n",
       "52427     Rosa d'Oro 2011 Trigrammaton Red (California) ...\n",
       "108626    Peltier 2012 Hybrid Pinot Noir (Lodi) This bas...\n",
       "85867     Browne Family Vineyards 2012 Cabernet Sauvigno...\n",
       "123457    SeaGlass 2010 Sauvignon Blanc (Santa Barbara C...\n",
       "20404     Bellangelo 2013 1866 Reserve Gewürztraminer (S...\n",
       "48238     Beringer 2014 Luminus Chardonnay (Oak Knoll Di...\n",
       "90268     Black Box 2009 3 L Pinot Grigio (California) T...\n",
       "55293     Brennan 2012 Super Nero Nero d'Avola (Texas) B...\n",
       "118772    Ludwig 2008 Hillside Vineyard Riesling (Santa ...\n",
       "65581     Barden 2014 Fonte White (Sta. Rita Hills) Very...\n",
       "77079     Kynsi 2013 Bien Nacido Vineyard Pinot Noir (Sa...\n",
       "59150     Mount Pleasant Winery 2010 Estates Vidal Blanc...\n",
       "46963     Pedroncelli 2011 Vintage Selection Chardonnay ...\n",
       "                                ...                        \n",
       "107365    Bouchaine 2012 Pinot Noir (Carneros-Napa Valle...\n",
       "83472     Ravenswood 2008 Old Hill Zinfandel (Sonoma Val...\n",
       "19639     Fox Run 2006 Reserve Chardonnay (Finger Lakes)...\n",
       "41077     Force of Nature 2013 Mossfire Ranch Zinfandel ...\n",
       "116       R2 2013 Camp 4 Vineyard Grenache Blanc (Santa ...\n",
       "102894    Longoria 2012 Pinot Noir (Sta. Rita Hills) Her...\n",
       "92610     McIntyre Vineyards NV L'Homme Qui Ris Sparklin...\n",
       "25798     Carlisle 2014 Two Acres Red (Russian River Val...\n",
       "53497     Claudia Springs 1998 Pinot Noir (Mendocino) Wi...\n",
       "72408     Covey Run 2008 Reserve Late Harvest Riesling (...\n",
       "127215    Boeschen Vineyards 2013 Terraces Red (Napa Val...\n",
       "37793     Lockwood 2013 Merlot (San Lucas) Dust, black c...\n",
       "63756     Arista 2014 Ritchie Vineyard Chardonnay (Russi...\n",
       "96092     Gloria Ferrer NV Blanc de Noirs Sparkling (Car...\n",
       "82073     Osprey's Dominion 2013 Reserve Pinot Gris (Nor...\n",
       "34118     Shea 2013 Block 7 Pinot Noir (Willamette Valle...\n",
       "20708     Rideau 2013 Siempre Happy Canyon Vineyard Malb...\n",
       "26988     Schug 2011 Estate Pinot Noir (Carneros) This P...\n",
       "77992     Hosmer 2016 Dry Cabernet Franc Rosé (Finger La...\n",
       "123016    Hazlitt 1852 Vineyards 2012 Barrel Fermented C...\n",
       "9270      The Federalist 2014 Chardonnay (Santa Barbara ...\n",
       "105546    Decroux 2015 John Sebastiano Vineyard Pinot No...\n",
       "74942     Barons 2010 Cabernet Sauvignon (Red Mountain) ...\n",
       "26218     Romililly 2012 Monte Rosso Vineyard Zinfandel ...\n",
       "89371     Novy 2010 Sauvignon Blanc (Russian River Valle...\n",
       "89427     Syncline 2011 Mourvèdre (Columbia Valley (WA))...\n",
       "76063     Jarvis 2012 Estate Grown Cave Fermented Cabern...\n",
       "105217    Hyatt 1998 Ice Wine Black Muscat (Yakima Valle...\n",
       "59371     Shaw 2010 Reserve Cabernet Sauvignon (Finger L...\n",
       "123826    Puccioni 2013 Old Vine Zinfandel (Dry Creek Va...\n",
       "Name: all_text, Length: 19076, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert = [np.squeeze(np.average(np.array(nlp(t)),axis=1), axis=0) for t in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_bert, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6555470917138552"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(LinearRegression(), X_train, y_train)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear_regression = make_pipeline(LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('linearregression',\n",
       "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                  normalize=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_linear_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6709984511745875"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_linear_regression.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT model produced the best results compared to the Word2Vec, Glove, and FastText models that only used the text features. But our BOW+Word2Vec+Non-text features model and our BOW+Word2Vec models were still better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would be to combine the BERT + BOW + non-text features which could possibly produce the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important point is that the BERT model was not fine-tuned to our text data which contained lots of jargon and vocabulary centered around wine. Fine-tuning the BERT model may produce best performing and lightest model of all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempts at fine-tuning the BERT model below. Not completed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6xTLrWgkOyX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig, AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EV2k0zMTkOyy"
   },
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"winemag-data-130k-v2.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4pmEQjhkOzE"
   },
   "outputs": [],
   "source": [
    "df = df_all[df_all[\"country\"] == \"US\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "PJj2iYPCkOzO",
    "outputId": "52a033a4-9280-426b-d8ea-e038d088c61e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['all_text'] = df['title'].astype(str) + \" \" +df['description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLiGS-6_kOzb"
   },
   "outputs": [],
   "source": [
    "y = df['points'].values\n",
    "X = df['all_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSAlLvbk1h_g"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "85c11b32f5f641e9861b0d403d4eda5c",
      "0e2b223f197c4bc98f42a4a9285a4b25",
      "5111e0e4ff084409b2a02a9a55a8c9e2",
      "473c323bebb542f58e39a6a8984052ec",
      "3774da6f992848b7b169fdcf0299d84e",
      "f5996f3a52854daf80f7416da7ccfc1d",
      "d0b062a53be3489f9f3a9dde393d8d5a",
      "cbde9eee24a2468da17863cfdad50aad"
     ]
    },
    "colab_type": "code",
    "id": "pf5P6JAzeOHl",
    "outputId": "29260003-81d0-4c45-b0cf-56e6992586fa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c11b32f5f641e9861b0d403d4eda5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "X_ids = [tokenizer.encode(doc, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True) for doc in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6oUwUFBoiZMm"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_ids, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_cBu9t7kBy1"
   },
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_valid = torch.tensor(X_valid)\n",
    "y_valid = torch.tensor(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzAA38QAlVmU"
   },
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxCiOq7cla6X"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(X_valid, y_valid)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7lvYerfEoWGt"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115,
     "referenced_widgets": [
      "689c6f05b8cc4800bb9ab43448245c19",
      "3f569ff122e84b82ad3fcf3bf87f9443",
      "4319bc473f584366a34544ebd37dda48",
      "f97afae6f68543a2a7bd9e37c5c327ee",
      "e007b3e8ccf949fa9f8ba400bf96f087",
      "13f99d9885a84df7ac94402f46727027",
      "f65a38be75ac430490f1771fc9f58843",
      "f7cfc55c865746baa90ebb9c0f347419",
      "b4ce24a5f6124a05abaf43e93592d182",
      "595cd2191e2d4a569a6065d36871f864",
      "21f29de77b5b40feaa192ea65ea28e85",
      "aaf377061d0f45c2a81734397edd5dc9",
      "1782cae8f85c48e782ae993b70c790ad",
      "e7a6117bb9ce47dc8c539e9315e03ddd",
      "2566991d35cc43ea9274cfc31a127ddf",
      "1b1d0a725697410798964262d4c38546"
     ]
    },
    "colab_type": "code",
    "id": "qykGlpmgl5i-",
    "outputId": "7c6279db-652b-4f7d-fb37-a7eb8384271a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689c6f05b8cc4800bb9ab43448245c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ce24a5f6124a05abaf43e93592d182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels= 1).to(device)\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels= 1)\n",
    "\n",
    "# Parameters:\n",
    "lr = 2e-5\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 3\n",
    "\n",
    "num_warmup_steps = 0\n",
    "num_training_steps = len(train_dataloader)*epochs\n",
    "\n",
    "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=adam_epsilon, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
    "criterion = torch.nn.MSELoss()  # loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "2sLcBMoSoeXD",
    "outputId": "8a7830f7-f852-4dd4-853e-8c1a73205d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 0 epochs...\n",
      "\n",
      "\tCurrent Learning rate:  1.3333333333333333e-05\n",
      "\n",
      "\tAverage Training loss: 1155.7368974479211\n",
      "\n",
      "\tR2 Score: -0.25443925491414565\n",
      "Running 1 epochs...\n"
     ]
    }
   ],
   "source": [
    "## Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "learning_rate = []\n",
    "\n",
    "# Gradients gets accumulated by default\n",
    "model.zero_grad()\n",
    "\n",
    "# tnrange is a tqdm wrapper around the normal python range\n",
    "for e in range(epochs):\n",
    "  print(f'Running {e} epochs...')\n",
    "  # Calculate total loss for this epoch\n",
    "  batch_loss = 0\n",
    "\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "    \n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_labels = batch\n",
    "\n",
    "    # print(b_input_ids)\n",
    "    # print(b_labels)\n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, labels=b_labels.float())\n",
    "    # print(outputs[0])\n",
    "    #loss = criterion(outputs, b_labels)\n",
    "    loss = outputs[0]\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip the norm of the gradients to 1.0\n",
    "    # Gradient clipping is not in AdamW anymore\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update learning rate schedule\n",
    "    scheduler.step()\n",
    "\n",
    "    # Clear the previous accumulated gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Update tracking variables\n",
    "    batch_loss += loss.item()\n",
    "\n",
    "  # Calculate the average loss over the training data.\n",
    "  avg_train_loss = batch_loss / len(train_dataloader)\n",
    "\n",
    "  #store the current learning rate\n",
    "  for param_group in optimizer.param_groups:\n",
    "    print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n",
    "    learning_rate.append(param_group['lr'])\n",
    "    \n",
    "  train_loss_set.append(avg_train_loss)\n",
    "  print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_r2_score, nb_eval_steps = 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids,b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None)\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[0].to('cpu').numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    pred_flat = logits.flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "\n",
    "    tmp_eval_r2_score = r2_score(labels_flat,pred_flat)\n",
    "    \n",
    "    eval_r2_score += tmp_eval_r2_score\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(F'\\n\\tR2 Score: {eval_r2_score/nb_eval_steps}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
