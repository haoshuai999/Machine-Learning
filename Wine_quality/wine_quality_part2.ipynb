{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied Machine Learning\n",
    "# HW4\n",
    "\n",
    "### Shuai Hao (sh3831), Eugene M. Joseph (emj2152)\n",
    "\n",
    "## Predict wine quality from review texts\n",
    "This notebook explores models built using features vectors from Glove, Word2Vec, and FastText word embeddings W using [the wine reviews data](https://www.kaggle.com/zynicide/wine-reviews) from Kaggle. The data were scraped on November 22nd, 2017. For this task, only the wine from the US are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Word Vectors [50pts]\n",
    "Use a pretrained word-embedding (word2vec, glove or fasttext) for featurization instead of the bag-of-words model. Does this improve classification? How about combining the embedded words with the BoW model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets load all our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, Normalizer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, StandardScaler, PolynomialFeatures, FunctionTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.impute import SimpleImputer, KNNImputer \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer, enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "import seaborn as sns\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\",disable=[\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"winemag-data-130k-v2.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all[df_all[\"country\"] == \"US\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['all_text'] = df['title'].astype(str) + \" \" +df['description'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To recap, here is our best model from part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['points']\n",
    "X = df.drop(columns=['country','points', 'taster_twitter_handle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = X_train.dtypes == 'object'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_continuous = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "preprocess_target = make_pipeline(\n",
    "    TargetEncoder(),\n",
    ")\n",
    "\n",
    "preprocess_dummy = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value=\"NA\"),\n",
    "    OneHotEncoder(handle_unknown='ignore')\n",
    ")\n",
    "\n",
    "proprocess_word = make_pipeline(\n",
    "    CountVectorizer(ngram_range=(1, 2), min_df = 1),\n",
    "    TfidfTransformer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = make_column_transformer(\n",
    "    (preprocess_continuous, ~categorical),\n",
    "    (preprocess_target, ['designation','region_1','variety','winery']),\n",
    "    (preprocess_dummy, ['province','region_2','taster_name']),\n",
    "    (proprocess_word, 'all_text'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear_regression = make_pipeline(preprocess, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7672466043879365"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(model_linear_regression, X_train, y_train)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('columntransformer',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('pipeline-1',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value=None,\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='median',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('standardscaler',\n",
       "                                                                   Standard...\n",
       "                                                                                   stop_words=None,\n",
       "                                                                                   strip_accents=None,\n",
       "                                                                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                                   tokenizer=None,\n",
       "                                                                                   vocabulary=None)),\n",
       "                                                                  ('tfidftransformer',\n",
       "                                                                   TfidfTransformer(norm='l2',\n",
       "                                                                                    smooth_idf=True,\n",
       "                                                                                    sublinear_tf=False,\n",
       "                                                                                    use_idf=True))],\n",
       "                                                           verbose=False),\n",
       "                                                  'all_text')],\n",
       "                                   verbose=False)),\n",
       "                ('linearregression',\n",
       "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                  normalize=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_linear_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775435040275117"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_linear_regression.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First,  lets try building a model that just using the Word2Vec features from Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['points']\n",
    "X = df['all_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_w2v = [nlp(d).vector for d in X]\n",
    "X_w2v = np.vstack(docs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54504, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_w2v, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5884998394991272"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(LinearRegression(), X_train, y_train)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear_regression = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5928605747883822"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_linear_regression.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores using Word2Vec features were lower than the best scores we received from part 1. That being said, this model was significantly smaller and much faster to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try again using Glove embeddings\n",
    "The glove word embeddings we picked were trained using Wikepedia and Gigaword. We downloaded the 300 dimension pre-trained word vectors from [glove's website](http://nlp.stanford.edu/data/glove.6B.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title'].astype(str) + \" \" +df['description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = \"D:/Natural-Language-Processing/hw2/resources/glove.6B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_glove = pd.read_csv(glove_path, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove = {key: val.values for key, val in df_glove.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_docs_train = []\n",
    "for doc in X_train:\n",
    "    doc_embeddings = []\n",
    "    for word in doc.split(\" \"):\n",
    "        if word in glove.keys():\n",
    "            doc_embeddings.append(glove[word])\n",
    "    doc_embeddings = np.asarray(doc_embeddings).mean(axis=0)\n",
    "    glove_docs_train.append(doc_embeddings)\n",
    "X_train_vector = np.vstack(glove_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40878, 300)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4704118742016389"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(LinearRegression(), X_train_vector, y_train)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear_regression = LinearRegression().fit(X_train_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_docs_test = []\n",
    "for doc in X_test:\n",
    "    doc_embeddings = []\n",
    "    for word in doc.split(\" \"):\n",
    "        if word in glove.keys():\n",
    "            doc_embeddings.append(glove[word])\n",
    "    doc_embeddings = np.asarray(doc_embeddings).mean(axis=0)\n",
    "    glove_docs_test.append(doc_embeddings)\n",
    "X_test_vector = np.vstack(glove_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4634534419856774"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_linear_regression.score(X_test_vector, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score using Glove features is lower than the word2vec features. Now lets try again with FastText embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try once again using FastText embeddings\n",
    "The fasttext word embeddings we used were trained on Common Crawl. We downloaded the 300 dimension pre-trained word vectors from [fasttext's website](https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title'].astype(str) + \" \" +df['description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_path = \"D:/Natural-Language-Processing/hw2/resources/crawl-300d-2M.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fasttext = pd.read_csv(fasttext_path, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "fasttext = {key: val.values for key, val in df_fasttext.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_docs_train = []\n",
    "for doc in X_train:\n",
    "    doc_embeddings = []\n",
    "    for word in doc.split(\" \"):\n",
    "        if word in fasttext.keys():\n",
    "            doc_embeddings.append(fasttext[word])\n",
    "    doc_embeddings = np.asarray(doc_embeddings).mean(axis=0)\n",
    "    fasttext_docs_train.append(doc_embeddings)\n",
    "X_train_vector = np.vstack(fasttext_docs_train)\n",
    "# glove_docs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40878, 300)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5544029996406648"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(LinearRegression(), X_train_vector, y_train)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear_regression = LinearRegression().fit(X_train_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_docs_test = []\n",
    "for doc in X_test:\n",
    "    doc_embeddings = []\n",
    "    for word in doc.split(\" \"):\n",
    "        if word in fasttext.keys():\n",
    "            doc_embeddings.append(fasttext[word])\n",
    "    doc_embeddings = np.asarray(doc_embeddings).mean(axis=0)\n",
    "    fasttext_docs_test.append(doc_embeddings)\n",
    "X_test_vector = np.vstack(fasttext_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5496359611002772"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_linear_regression.score(X_test_vector, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText did better than Glove but Word2Vec is still the best of the three.  \n",
    "Now let's try combining the our BOW features with the Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['points']\n",
    "X = df[['all_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54504, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = X.join(pd.DataFrame(X_w2v, columns = [\"w2v_%d\" % (i) for i in range(300)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54504, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54504, 301)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "      <th>w2v_0</th>\n",
       "      <th>w2v_1</th>\n",
       "      <th>w2v_2</th>\n",
       "      <th>w2v_3</th>\n",
       "      <th>w2v_4</th>\n",
       "      <th>w2v_5</th>\n",
       "      <th>w2v_6</th>\n",
       "      <th>w2v_7</th>\n",
       "      <th>w2v_8</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_290</th>\n",
       "      <th>w2v_291</th>\n",
       "      <th>w2v_292</th>\n",
       "      <th>w2v_293</th>\n",
       "      <th>w2v_294</th>\n",
       "      <th>w2v_295</th>\n",
       "      <th>w2v_296</th>\n",
       "      <th>w2v_297</th>\n",
       "      <th>w2v_298</th>\n",
       "      <th>w2v_299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley) ...</td>\n",
       "      <td>-0.059305</td>\n",
       "      <td>0.178837</td>\n",
       "      <td>-0.103683</td>\n",
       "      <td>-0.081007</td>\n",
       "      <td>0.114283</td>\n",
       "      <td>0.169241</td>\n",
       "      <td>0.038487</td>\n",
       "      <td>-0.107372</td>\n",
       "      <td>-0.093617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075751</td>\n",
       "      <td>0.118462</td>\n",
       "      <td>-0.121482</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>-0.095376</td>\n",
       "      <td>-0.048598</td>\n",
       "      <td>0.007584</td>\n",
       "      <td>-0.155806</td>\n",
       "      <td>-0.029952</td>\n",
       "      <td>0.001130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>\n",
       "      <td>-0.123089</td>\n",
       "      <td>0.166517</td>\n",
       "      <td>-0.015784</td>\n",
       "      <td>-0.149360</td>\n",
       "      <td>0.170512</td>\n",
       "      <td>0.121510</td>\n",
       "      <td>0.061644</td>\n",
       "      <td>0.007022</td>\n",
       "      <td>-0.008923</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056513</td>\n",
       "      <td>0.216762</td>\n",
       "      <td>-0.054715</td>\n",
       "      <td>-0.029836</td>\n",
       "      <td>-0.140443</td>\n",
       "      <td>-0.084087</td>\n",
       "      <td>0.016938</td>\n",
       "      <td>-0.170598</td>\n",
       "      <td>-0.004161</td>\n",
       "      <td>0.075681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>\n",
       "      <td>-0.119515</td>\n",
       "      <td>0.158756</td>\n",
       "      <td>-0.012404</td>\n",
       "      <td>-0.112647</td>\n",
       "      <td>0.152178</td>\n",
       "      <td>0.069793</td>\n",
       "      <td>-0.013483</td>\n",
       "      <td>-0.062202</td>\n",
       "      <td>-0.027927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062307</td>\n",
       "      <td>0.109630</td>\n",
       "      <td>-0.107260</td>\n",
       "      <td>-0.113588</td>\n",
       "      <td>-0.171410</td>\n",
       "      <td>-0.097073</td>\n",
       "      <td>0.051592</td>\n",
       "      <td>-0.213532</td>\n",
       "      <td>-0.068529</td>\n",
       "      <td>0.031298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kirkland Signature 2011 Mountain Cuvée Caberne...</td>\n",
       "      <td>-0.021261</td>\n",
       "      <td>0.142845</td>\n",
       "      <td>-0.031244</td>\n",
       "      <td>-0.162984</td>\n",
       "      <td>0.210119</td>\n",
       "      <td>0.112590</td>\n",
       "      <td>0.089680</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>-0.100519</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053804</td>\n",
       "      <td>0.095881</td>\n",
       "      <td>-0.018253</td>\n",
       "      <td>-0.053935</td>\n",
       "      <td>-0.128425</td>\n",
       "      <td>-0.008587</td>\n",
       "      <td>0.071964</td>\n",
       "      <td>-0.138455</td>\n",
       "      <td>-0.017377</td>\n",
       "      <td>0.001073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Louis M. Martini 2012 Cabernet Sauvignon (Alex...</td>\n",
       "      <td>-0.086759</td>\n",
       "      <td>0.226323</td>\n",
       "      <td>-0.055962</td>\n",
       "      <td>-0.272232</td>\n",
       "      <td>0.188816</td>\n",
       "      <td>0.227645</td>\n",
       "      <td>-0.039569</td>\n",
       "      <td>-0.014617</td>\n",
       "      <td>-0.131085</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071923</td>\n",
       "      <td>0.197889</td>\n",
       "      <td>-0.136351</td>\n",
       "      <td>-0.076265</td>\n",
       "      <td>-0.269704</td>\n",
       "      <td>-0.116376</td>\n",
       "      <td>0.105994</td>\n",
       "      <td>-0.301777</td>\n",
       "      <td>-0.055862</td>\n",
       "      <td>-0.049494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             all_text     w2v_0     w2v_1  \\\n",
       "2   Rainstorm 2013 Pinot Gris (Willamette Valley) ... -0.059305  0.178837   \n",
       "3   St. Julian 2013 Reserve Late Harvest Riesling ... -0.123089  0.166517   \n",
       "4   Sweet Cheeks 2012 Vintner's Reserve Wild Child... -0.119515  0.158756   \n",
       "10  Kirkland Signature 2011 Mountain Cuvée Caberne... -0.021261  0.142845   \n",
       "12  Louis M. Martini 2012 Cabernet Sauvignon (Alex... -0.086759  0.226323   \n",
       "\n",
       "       w2v_2     w2v_3     w2v_4     w2v_5     w2v_6     w2v_7     w2v_8  ...  \\\n",
       "2  -0.103683 -0.081007  0.114283  0.169241  0.038487 -0.107372 -0.093617  ...   \n",
       "3  -0.015784 -0.149360  0.170512  0.121510  0.061644  0.007022 -0.008923  ...   \n",
       "4  -0.012404 -0.112647  0.152178  0.069793 -0.013483 -0.062202 -0.027927  ...   \n",
       "10 -0.031244 -0.162984  0.210119  0.112590  0.089680  0.026466 -0.100519  ...   \n",
       "12 -0.055962 -0.272232  0.188816  0.227645 -0.039569 -0.014617 -0.131085  ...   \n",
       "\n",
       "     w2v_290   w2v_291   w2v_292   w2v_293   w2v_294   w2v_295   w2v_296  \\\n",
       "2  -0.075751  0.118462 -0.121482 -0.000904 -0.095376 -0.048598  0.007584   \n",
       "3  -0.056513  0.216762 -0.054715 -0.029836 -0.140443 -0.084087  0.016938   \n",
       "4  -0.062307  0.109630 -0.107260 -0.113588 -0.171410 -0.097073  0.051592   \n",
       "10 -0.053804  0.095881 -0.018253 -0.053935 -0.128425 -0.008587  0.071964   \n",
       "12 -0.071923  0.197889 -0.136351 -0.076265 -0.269704 -0.116376  0.105994   \n",
       "\n",
       "     w2v_297   w2v_298   w2v_299  \n",
       "2  -0.155806 -0.029952  0.001130  \n",
       "3  -0.170598 -0.004161  0.075681  \n",
       "4  -0.213532 -0.068529  0.031298  \n",
       "10 -0.138455 -0.017377  0.001073  \n",
       "12 -0.301777 -0.055862 -0.049494  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_col_names = X_full.columns[11:311].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w2v_10',\n",
       " 'w2v_11',\n",
       " 'w2v_12',\n",
       " 'w2v_13',\n",
       " 'w2v_14',\n",
       " 'w2v_15',\n",
       " 'w2v_16',\n",
       " 'w2v_17',\n",
       " 'w2v_18',\n",
       " 'w2v_19',\n",
       " 'w2v_20',\n",
       " 'w2v_21',\n",
       " 'w2v_22',\n",
       " 'w2v_23',\n",
       " 'w2v_24',\n",
       " 'w2v_25',\n",
       " 'w2v_26',\n",
       " 'w2v_27',\n",
       " 'w2v_28',\n",
       " 'w2v_29',\n",
       " 'w2v_30',\n",
       " 'w2v_31',\n",
       " 'w2v_32',\n",
       " 'w2v_33',\n",
       " 'w2v_34',\n",
       " 'w2v_35',\n",
       " 'w2v_36',\n",
       " 'w2v_37',\n",
       " 'w2v_38',\n",
       " 'w2v_39',\n",
       " 'w2v_40',\n",
       " 'w2v_41',\n",
       " 'w2v_42',\n",
       " 'w2v_43',\n",
       " 'w2v_44',\n",
       " 'w2v_45',\n",
       " 'w2v_46',\n",
       " 'w2v_47',\n",
       " 'w2v_48',\n",
       " 'w2v_49',\n",
       " 'w2v_50',\n",
       " 'w2v_51',\n",
       " 'w2v_52',\n",
       " 'w2v_53',\n",
       " 'w2v_54',\n",
       " 'w2v_55',\n",
       " 'w2v_56',\n",
       " 'w2v_57',\n",
       " 'w2v_58',\n",
       " 'w2v_59',\n",
       " 'w2v_60',\n",
       " 'w2v_61',\n",
       " 'w2v_62',\n",
       " 'w2v_63',\n",
       " 'w2v_64',\n",
       " 'w2v_65',\n",
       " 'w2v_66',\n",
       " 'w2v_67',\n",
       " 'w2v_68',\n",
       " 'w2v_69',\n",
       " 'w2v_70',\n",
       " 'w2v_71',\n",
       " 'w2v_72',\n",
       " 'w2v_73',\n",
       " 'w2v_74',\n",
       " 'w2v_75',\n",
       " 'w2v_76',\n",
       " 'w2v_77',\n",
       " 'w2v_78',\n",
       " 'w2v_79',\n",
       " 'w2v_80',\n",
       " 'w2v_81',\n",
       " 'w2v_82',\n",
       " 'w2v_83',\n",
       " 'w2v_84',\n",
       " 'w2v_85',\n",
       " 'w2v_86',\n",
       " 'w2v_87',\n",
       " 'w2v_88',\n",
       " 'w2v_89',\n",
       " 'w2v_90',\n",
       " 'w2v_91',\n",
       " 'w2v_92',\n",
       " 'w2v_93',\n",
       " 'w2v_94',\n",
       " 'w2v_95',\n",
       " 'w2v_96',\n",
       " 'w2v_97',\n",
       " 'w2v_98',\n",
       " 'w2v_99',\n",
       " 'w2v_100',\n",
       " 'w2v_101',\n",
       " 'w2v_102',\n",
       " 'w2v_103',\n",
       " 'w2v_104',\n",
       " 'w2v_105',\n",
       " 'w2v_106',\n",
       " 'w2v_107',\n",
       " 'w2v_108',\n",
       " 'w2v_109',\n",
       " 'w2v_110',\n",
       " 'w2v_111',\n",
       " 'w2v_112',\n",
       " 'w2v_113',\n",
       " 'w2v_114',\n",
       " 'w2v_115',\n",
       " 'w2v_116',\n",
       " 'w2v_117',\n",
       " 'w2v_118',\n",
       " 'w2v_119',\n",
       " 'w2v_120',\n",
       " 'w2v_121',\n",
       " 'w2v_122',\n",
       " 'w2v_123',\n",
       " 'w2v_124',\n",
       " 'w2v_125',\n",
       " 'w2v_126',\n",
       " 'w2v_127',\n",
       " 'w2v_128',\n",
       " 'w2v_129',\n",
       " 'w2v_130',\n",
       " 'w2v_131',\n",
       " 'w2v_132',\n",
       " 'w2v_133',\n",
       " 'w2v_134',\n",
       " 'w2v_135',\n",
       " 'w2v_136',\n",
       " 'w2v_137',\n",
       " 'w2v_138',\n",
       " 'w2v_139',\n",
       " 'w2v_140',\n",
       " 'w2v_141',\n",
       " 'w2v_142',\n",
       " 'w2v_143',\n",
       " 'w2v_144',\n",
       " 'w2v_145',\n",
       " 'w2v_146',\n",
       " 'w2v_147',\n",
       " 'w2v_148',\n",
       " 'w2v_149',\n",
       " 'w2v_150',\n",
       " 'w2v_151',\n",
       " 'w2v_152',\n",
       " 'w2v_153',\n",
       " 'w2v_154',\n",
       " 'w2v_155',\n",
       " 'w2v_156',\n",
       " 'w2v_157',\n",
       " 'w2v_158',\n",
       " 'w2v_159',\n",
       " 'w2v_160',\n",
       " 'w2v_161',\n",
       " 'w2v_162',\n",
       " 'w2v_163',\n",
       " 'w2v_164',\n",
       " 'w2v_165',\n",
       " 'w2v_166',\n",
       " 'w2v_167',\n",
       " 'w2v_168',\n",
       " 'w2v_169',\n",
       " 'w2v_170',\n",
       " 'w2v_171',\n",
       " 'w2v_172',\n",
       " 'w2v_173',\n",
       " 'w2v_174',\n",
       " 'w2v_175',\n",
       " 'w2v_176',\n",
       " 'w2v_177',\n",
       " 'w2v_178',\n",
       " 'w2v_179',\n",
       " 'w2v_180',\n",
       " 'w2v_181',\n",
       " 'w2v_182',\n",
       " 'w2v_183',\n",
       " 'w2v_184',\n",
       " 'w2v_185',\n",
       " 'w2v_186',\n",
       " 'w2v_187',\n",
       " 'w2v_188',\n",
       " 'w2v_189',\n",
       " 'w2v_190',\n",
       " 'w2v_191',\n",
       " 'w2v_192',\n",
       " 'w2v_193',\n",
       " 'w2v_194',\n",
       " 'w2v_195',\n",
       " 'w2v_196',\n",
       " 'w2v_197',\n",
       " 'w2v_198',\n",
       " 'w2v_199',\n",
       " 'w2v_200',\n",
       " 'w2v_201',\n",
       " 'w2v_202',\n",
       " 'w2v_203',\n",
       " 'w2v_204',\n",
       " 'w2v_205',\n",
       " 'w2v_206',\n",
       " 'w2v_207',\n",
       " 'w2v_208',\n",
       " 'w2v_209',\n",
       " 'w2v_210',\n",
       " 'w2v_211',\n",
       " 'w2v_212',\n",
       " 'w2v_213',\n",
       " 'w2v_214',\n",
       " 'w2v_215',\n",
       " 'w2v_216',\n",
       " 'w2v_217',\n",
       " 'w2v_218',\n",
       " 'w2v_219',\n",
       " 'w2v_220',\n",
       " 'w2v_221',\n",
       " 'w2v_222',\n",
       " 'w2v_223',\n",
       " 'w2v_224',\n",
       " 'w2v_225',\n",
       " 'w2v_226',\n",
       " 'w2v_227',\n",
       " 'w2v_228',\n",
       " 'w2v_229',\n",
       " 'w2v_230',\n",
       " 'w2v_231',\n",
       " 'w2v_232',\n",
       " 'w2v_233',\n",
       " 'w2v_234',\n",
       " 'w2v_235',\n",
       " 'w2v_236',\n",
       " 'w2v_237',\n",
       " 'w2v_238',\n",
       " 'w2v_239',\n",
       " 'w2v_240',\n",
       " 'w2v_241',\n",
       " 'w2v_242',\n",
       " 'w2v_243',\n",
       " 'w2v_244',\n",
       " 'w2v_245',\n",
       " 'w2v_246',\n",
       " 'w2v_247',\n",
       " 'w2v_248',\n",
       " 'w2v_249',\n",
       " 'w2v_250',\n",
       " 'w2v_251',\n",
       " 'w2v_252',\n",
       " 'w2v_253',\n",
       " 'w2v_254',\n",
       " 'w2v_255',\n",
       " 'w2v_256',\n",
       " 'w2v_257',\n",
       " 'w2v_258',\n",
       " 'w2v_259',\n",
       " 'w2v_260',\n",
       " 'w2v_261',\n",
       " 'w2v_262',\n",
       " 'w2v_263',\n",
       " 'w2v_264',\n",
       " 'w2v_265',\n",
       " 'w2v_266',\n",
       " 'w2v_267',\n",
       " 'w2v_268',\n",
       " 'w2v_269',\n",
       " 'w2v_270',\n",
       " 'w2v_271',\n",
       " 'w2v_272',\n",
       " 'w2v_273',\n",
       " 'w2v_274',\n",
       " 'w2v_275',\n",
       " 'w2v_276',\n",
       " 'w2v_277',\n",
       " 'w2v_278',\n",
       " 'w2v_279',\n",
       " 'w2v_280',\n",
       " 'w2v_281',\n",
       " 'w2v_282',\n",
       " 'w2v_283',\n",
       " 'w2v_284',\n",
       " 'w2v_285',\n",
       " 'w2v_286',\n",
       " 'w2v_287',\n",
       " 'w2v_288',\n",
       " 'w2v_289',\n",
       " 'w2v_290',\n",
       " 'w2v_291',\n",
       " 'w2v_292',\n",
       " 'w2v_293',\n",
       " 'w2v_294',\n",
       " 'w2v_295',\n",
       " 'w2v_296',\n",
       " 'w2v_297',\n",
       " 'w2v_298',\n",
       " 'w2v_299']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_glove = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),\n",
    ")\n",
    "\n",
    "preprocess_word = make_pipeline(\n",
    "    CountVectorizer(ngram_range=(1, 2), min_df = 1),\n",
    "    TfidfTransformer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = make_column_transformer(\n",
    "    (preprocess_glove, w2v_col_names),\n",
    "    (preprocess_word, 'all_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear_regression = make_pipeline(preprocess, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40878, 301)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40878,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7496046199731424"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(model_linear_regression, X_train, y_train)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('columntransformer',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('pipeline-1',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value=None,\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='median',\n",
       "                                                                                 verbose=0))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['w2v_10',...\n",
       "                                                                                   stop_words=None,\n",
       "                                                                                   strip_accents=None,\n",
       "                                                                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                                   tokenizer=None,\n",
       "                                                                                   vocabulary=None)),\n",
       "                                                                  ('tfidftransformer',\n",
       "                                                                   TfidfTransformer(norm='l2',\n",
       "                                                                                    smooth_idf=True,\n",
       "                                                                                    sublinear_tf=False,\n",
       "                                                                                    use_idf=True))],\n",
       "                                                           verbose=False),\n",
       "                                                  'all_text')],\n",
       "                                   verbose=False)),\n",
       "                ('linearregression',\n",
       "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                  normalize=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_linear_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7613436995899436"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_linear_regression.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation and test scores seem to have actually decreased a little bit when combining the Word2Vec and BOW features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + BOW + Other available features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['points']\n",
    "X = df.drop(columns=['country','points', 'taster_twitter_handle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54504, 11)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = X.join(pd.DataFrame(X_w2v, columns = [\"w2v_%d\" % (i) for i in range(300)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_continuous = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),\n",
    ")\n",
    "\n",
    "preprocess_glove = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "preprocess_target = make_pipeline(\n",
    "    TargetEncoder(),\n",
    ")\n",
    "\n",
    "preprocess_dummy = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value=\"NA\"),\n",
    "    OneHotEncoder(handle_unknown='ignore')\n",
    ")\n",
    "\n",
    "preprocess_word = make_pipeline(\n",
    "    CountVectorizer(ngram_range=(1, 2), min_df = 1),\n",
    "    TfidfTransformer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = make_column_transformer(\n",
    "    (preprocess_glove, w2v_col_names),\n",
    "    (preprocess_continuous, ['price']),\n",
    "    (preprocess_target, ['designation','region_1','variety','winery']),\n",
    "    (preprocess_dummy, ['province','region_2','taster_name']),\n",
    "    (preprocess_word, 'all_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear_regression = make_pipeline(preprocess, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40878, 311)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7672678103397905"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(model_linear_regression, X_train, y_train)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('columntransformer',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('pipeline-1',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value=None,\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='median',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('standardscaler',\n",
       "                                                                   Standard...\n",
       "                                                                                   stop_words=None,\n",
       "                                                                                   strip_accents=None,\n",
       "                                                                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                                   tokenizer=None,\n",
       "                                                                                   vocabulary=None)),\n",
       "                                                                  ('tfidftransformer',\n",
       "                                                                   TfidfTransformer(norm='l2',\n",
       "                                                                                    smooth_idf=True,\n",
       "                                                                                    sublinear_tf=False,\n",
       "                                                                                    use_idf=True))],\n",
       "                                                           verbose=False),\n",
       "                                                  'all_text')],\n",
       "                                   verbose=False)),\n",
       "                ('linearregression',\n",
       "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                  normalize=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_linear_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7753824251271151"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_linear_regression.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model improved slightly after adding in the other available features but it's still not really better than our best model from part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
